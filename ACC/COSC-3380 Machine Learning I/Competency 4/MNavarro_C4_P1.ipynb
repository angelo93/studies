{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michael Navarro: Competency 4 - Project 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Instructions\n",
    "\n",
    "Task 1: Complete Exercise 8 from Chapter 7 in Textbook:\n",
    ">* Load the MNIST data (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation,and 10,000 for testing). \n",
    ">* Then train various classifiers, such as a RandomForest classifier, an Extra-Trees classifier, and an SVM. \n",
    ">* Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. \n",
    ">* Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?\n",
    "\n",
    "Task 2: \n",
    ">* Repeat the previous task on a different dataset  (you can pick one of the datasets used in previous projects or pick a new one). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports & Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------\n",
    "#                                       Imports \n",
    "#----------------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "#----------------------------------- sklearn imports -----------------------------------\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "#                                   Global Constants\n",
    "#----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset from OpenML\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "mnist.target = mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data\n",
    "* 50K - Training\n",
    "* 10K - Validation\n",
    "* 10K - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, test_size=10000, random_state=42)\n",
    "\n",
    "# Test Set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Modeling Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Model Parameters\n",
    "random_forest_clf   = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "extra_trees_clf     = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
    "svm_clf             = LinearSVC(max_iter=100, tol=20, random_state=42)\n",
    "mlp_clf             = MLPClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of models and their respective names for iterative purposes\n",
    "estimators      = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]\n",
    "estimator_names = ['Random Forest Classifier', 'Extra Trees Classifier', 'Linear SVC', 'MLP Classifier']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Random Forest Classifier\n",
      "Training: Extra Trees Classifier\n",
      "Training: Linear SVC\n",
      "Training: MLP Classifier\n"
     ]
    }
   ],
   "source": [
    "# Train Models\n",
    "idx = 0\n",
    "for estimator in estimators:\n",
    "    print(f'Training: {estimator_names[idx]}')\n",
    "    # Train model\n",
    "    estimator.fit(X_train, y_train)\n",
    "    idx += 1\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest Classifier  Score:  0.9692\n",
      "Model: Extra Trees Classifier    Score:  0.9715\n",
      "Model: Linear SVC                Score:  0.8590\n",
      "Model: MLP Classifier            Score:  0.9577\n"
     ]
    }
   ],
   "source": [
    "# Display Scores of each model\n",
    "idx = 0\n",
    "for estimator in estimators:\n",
    "    print(f'Model: {estimator_names[idx]: <25} Score: {estimator.score(X_val, y_val): .4f}')\n",
    "    idx += 1\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least performant model was Linear SVC while the best performant model was Random Forest Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Voting Ensemble Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "named_estimators = [\n",
    "    (\"random_forest_clf\", random_forest_clf),\n",
    "    (\"extra_trees_clf\", extra_trees_clf),\n",
    "    (\"svm_clf\", svm_clf),\n",
    "    (\"mlp_clf\", mlp_clf),\n",
    "]\n",
    "\n",
    "voting_clf = VotingClassifier(named_estimators)\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "print() # Silence output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score: 0.9703\n"
     ]
    }
   ],
   "source": [
    "# Display Ensemble score \n",
    "ens_score = voting_clf.score(X_val, y_val)\n",
    "print(f'Ensemble Score: {ens_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForestClassifier(random_state=42), ExtraTreesClassifier(random_state=42)]\n"
     ]
    }
   ],
   "source": [
    "# \"Turn off\" two lowest performing models\n",
    "voting_clf.set_params(svm_clf=None)\n",
    "voting_clf.set_params(mlp_clf=None)\n",
    "\n",
    "# Remove Trained Estimators\n",
    "# We have to reference the same index due to order of operations\n",
    "del voting_clf.estimators_[2]   # SVM\n",
    "del voting_clf.estimators_[2]   # MLP\n",
    "\n",
    "# Show remaining estimators\n",
    "print(voting_clf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score: 0.9713\n"
     ]
    }
   ],
   "source": [
    "# Display Ensemble score \n",
    "ens_score = voting_clf.score(X_val, y_val)\n",
    "print(f'Ensemble Score: {ens_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminating the two lowest performing models resulted in a score lower than the highest model score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Voting: Soft Vs. Hard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score (Soft): 0.9719\n"
     ]
    }
   ],
   "source": [
    "voting_clf.voting = \"soft\"\n",
    "\n",
    "# Display Ensemble score \n",
    "ens_score = voting_clf.score(X_val, y_val)\n",
    "print(f'Ensemble Score (Soft): {ens_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the voting to \"soft\" yields the highest score so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison With Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score: 0.9719\n",
      "Model: Random Forest Classifier  Score:  0.9692 < 0.9719\n",
      "Model: Extra Trees Classifier    Score:  0.9715 < 0.9719\n"
     ]
    }
   ],
   "source": [
    "ens_score = voting_clf.score(X_val, y_val)\n",
    "print(f'Ensemble Score: {ens_score}')\n",
    "\n",
    "# Display Scores of each model\n",
    "idx = 0\n",
    "for estimator in voting_clf.estimators_:\n",
    "    tmp_score = estimator.score(X_val, y_val)\n",
    "    \n",
    "    tmp_operator = '<' if (tmp_score < ens_score) else '>'\n",
    "         \n",
    "    print(f'Model: {estimator_names[idx]: <25} Score: {tmp_score: .4f} {tmp_operator} {ens_score}')\n",
    "    idx += 1\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eliminating the two lowest performaing models, our soft ensemble score is higher than any one individual model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reinforce the methodology process from the first task, I chose to use another image based MNIST dataset. The fashion dataset was originally found on Kaggle, but after further resarch on the dataset, I found that it was also on OpenML. This data set in particular is well documented and maintained. Furthermore, the dataset itself also had 70,000 entries. By using this dataset, I was able to see how exactly the same methodology yielded different results due to the differing nature of the Fashion dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset from OpenML\n",
    "fashion_mnist = fetch_openml('Fashion-MNIST', version=1, as_frame=False)\n",
    "fashion_mnist.target = fashion_mnist.target.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    fashion_mnist.data, fashion_mnist.target, test_size=10000, random_state=42)\n",
    "\n",
    "# Test Set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Random Forest Classifier\n",
      "Training: Extra Trees Classifier\n",
      "Training: Linear SVC\n",
      "Training: MLP Classifier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Angel\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train Models\n",
    "idx = 0\n",
    "for estimator in estimators:                    # Estimators defined in Task 1\n",
    "    print(f'Training: {estimator_names[idx]}')  # Estimator names defined in Task 1\n",
    "    # Train model\n",
    "    estimator.fit(X_train, y_train)\n",
    "    idx += 1\n",
    "# end for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Random Forest Classifier  Score:  0.8845\n",
      "Model: Extra Trees Classifier    Score:  0.8844\n",
      "Model: Linear SVC                Score:  0.8127\n",
      "Model: MLP Classifier            Score:  0.8574\n"
     ]
    }
   ],
   "source": [
    "# Display Scores of each model\n",
    "idx = 0\n",
    "for estimator in estimators:\n",
    "    print(f'Model: {estimator_names[idx]: <25} Score: {estimator.score(X_val, y_val): .4f}')\n",
    "    idx += 1\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Angel\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:614: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "voting_clf = VotingClassifier(named_estimators) # named_estimators defined in Task 1\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "print() # Silence output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score: 0.8839\n"
     ]
    }
   ],
   "source": [
    "# Display Ensemble score \n",
    "ens_score = voting_clf.score(X_val, y_val)\n",
    "print(f'Ensemble Score: {ens_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForestClassifier(random_state=42), ExtraTreesClassifier(random_state=42), MLPClassifier(random_state=42)]\n"
     ]
    }
   ],
   "source": [
    "# \"Turn off\" lowest performing models\n",
    "voting_clf.set_params(svm_clf=None)\n",
    "\n",
    "# Remove Trained Estimator\n",
    "del voting_clf.estimators_[2]   # SVM\n",
    "\n",
    "# Show remaining estimators\n",
    "print(voting_clf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score: 0.889\n"
     ]
    }
   ],
   "source": [
    "# Display Ensemble score \n",
    "ens_score = voting_clf.score(X_val, y_val)\n",
    "print(f'Ensemble Score: {ens_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminating the lowest performing model (SVM) yields the best result thus far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score (Soft): 0.8864\n"
     ]
    }
   ],
   "source": [
    "voting_clf.voting = \"soft\"\n",
    "\n",
    "# Display Ensemble score \n",
    "ens_score = voting_clf.score(X_val, y_val)\n",
    "print(f'Ensemble Score (Soft): {ens_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time around the \"soft\" ensemble score was lower than the \"hard\" score. We will revert back to the \"hard\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score (Hard): 0.889\n"
     ]
    }
   ],
   "source": [
    "# Set voting back to 'hard'\n",
    "voting_clf.voting = \"hard\"\n",
    "\n",
    "# Display Ensemble score \n",
    "ens_score = voting_clf.score(X_val, y_val)\n",
    "print(f'Ensemble Score (Hard): {ens_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score: 0.889\n",
      "Model: Random Forest Classifier  Score:  0.8845 < 0.889\n",
      "Model: Extra Trees Classifier    Score:  0.8844 < 0.889\n",
      "Model: Linear SVC                Score:  0.8574 < 0.889\n"
     ]
    }
   ],
   "source": [
    "ens_score = voting_clf.score(X_val, y_val)\n",
    "print(f'Ensemble Score: {ens_score}')\n",
    "\n",
    "# tmp_names = estimator_names.remove('Linear SVC')\n",
    "\n",
    "# Display Scores of each model\n",
    "idx = 0\n",
    "for estimator in voting_clf.estimators_:\n",
    "    tmp_score = estimator.score(X_val, y_val)\n",
    "    \n",
    "    tmp_operator = '<' if (tmp_score < ens_score) else '>'\n",
    "         \n",
    "    print(f'Model: {estimator_names[idx]: <25} Score: {tmp_score: .4f} {tmp_operator} {ens_score}')\n",
    "    idx += 1\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using \"hard\" voting, our ensemble score is better than any single model score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What If: Remove MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[RandomForestClassifier(random_state=42), ExtraTreesClassifier(random_state=42)]\n"
     ]
    }
   ],
   "source": [
    "# \"Turn off\" lowest performing models\n",
    "voting_clf.set_params(mlp_clf=None)\n",
    "\n",
    "# Remove Trained Estimator\n",
    "del voting_clf.estimators_[2]   # MLP\n",
    "\n",
    "# Show remaining estimators\n",
    "print(voting_clf.estimators_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score: 0.8831\n"
     ]
    }
   ],
   "source": [
    "# Display Ensemble score \n",
    "ens_score = voting_clf.score(X_val, y_val)\n",
    "print(f'Ensemble Score: {ens_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing MLP actually reduced our previous ensemble score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score (Soft): 0.8856\n"
     ]
    }
   ],
   "source": [
    "voting_clf.voting = \"soft\"\n",
    "\n",
    "# Display Ensemble score \n",
    "ens_score = voting_clf.score(X_val, y_val)\n",
    "print(f'Ensemble Score (Soft): {ens_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interstingly, by removing MLP, the \"soft\" voting score is higher than \"hard\" voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Score: 0.8856\n",
      "Model: Random Forest Classifier  Score:  0.8845 < 0.8856\n",
      "Model: Extra Trees Classifier    Score:  0.8844 < 0.8856\n"
     ]
    }
   ],
   "source": [
    "ens_score = voting_clf.score(X_val, y_val)\n",
    "print(f'Ensemble Score: {ens_score}')\n",
    "\n",
    "# tmp_names = estimator_names.remove('Linear SVC')\n",
    "\n",
    "# Display Scores of each model\n",
    "idx = 0\n",
    "for estimator in voting_clf.estimators_:\n",
    "    tmp_score = estimator.score(X_val, y_val)\n",
    "    \n",
    "    tmp_operator = '<' if (tmp_score < ens_score) else '>'\n",
    "         \n",
    "    print(f'Model: {estimator_names[idx]: <25} Score: {tmp_score: .4f} {tmp_operator} {ens_score}')\n",
    "    idx += 1\n",
    "# end for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the ensemble score is still higher, the performance margin was reduced after removing MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found it very interesting how SVM performed the worst with both datasets. However, the most interesting thing I found was when I removed MLP in the second task. The removal of MLP resulted in a sort of \"turning the tables\" in the result score. The ensemble score was actually worse than when MLP was included and, alternatively, the \"soft\" voting score was higher than the \"hard\"voting score. This is the exact opposite of when MLP was included in the ensemble scoring."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "324e6329eac48fe6ab01ccd6319cfb623332e206da170c10b7c411bb22490aae"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
